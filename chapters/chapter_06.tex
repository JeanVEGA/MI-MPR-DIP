\chapter{Testing}\label{cha:testing}

	\section{Unit Testing}
	
	\cite{msdnunit}
	The primary goal of unit testing is to take the smallest piece of testable software in the application, isolate it from
	the remainder of the code, and determine whether it behaves exactly as expected. Each unit is tested separately
	before integrating them into modules to test the interfaces between modules.
	
	In other words, unit tests test class by class, method by method. If a tested class or method has a dependency, the
	dependency has to be stubbed, mocked, faked, dummied or spied. The five just named together form a set of test doubles.

	\subsection{\gls{TDD}}
	
	\cite{msdntdd}
	TDD is an advanced technique of using automated unit tests to drive the design of software
	and force decoupling of dependencies. The result of using this practice is a comprehensive suite of unit tests that can
	be run at any time to provide feedback that the software is still working. This technique is heavily emphasized by
	those using Agile development methodologies.
	
	These steps should be followed when doing TDD right:
	
	\begin{itemize}
		\item Understand the requirements of the story, work item, or feature that you are working on
		\item \textbf{Red} Create a test and make it fail
		\begin{itemize}
			\item Imagine how the new code should be called and write the test as if the code already existed.
			\item Create the new production code stub. Write just enough code so that it compiles.
			\item Run the test. It should fail. This is a calibration measure to ensure that your test is calling the correct
			code and that the code is not working by accident. This is a meaningful failure, and you expect it to fail.
		\end{itemize}
		\item \textbf{Green} Make the test pass by any means necessary
		\begin{itemize}
			\item Write the production code to make the test pass. Keep it simple.
			\item Some advocate the hard-coding of the expected return value first to verify that the test correctly detects
			success. This varies from practitioner to practitioner.
			\item If you've written the code so that the test passes as intended, you are finished. You do not have to write more
			code speculatively. The test is the objective definition of \uv{done.} The phrase \gls{YAGNI} is often used to veto
			unnecessary work. If new functionality is still needed, then another test is needed. Make this one test pass and
			continue.
			\item When the test passes, you might want to run all tests up to this point to build confidence that everything else
			is still working.
		\end{itemize}
		\item \textbf{Refactor} Change the code to remove duplication in your project and to improve the design while ensuring
		that all tests still pass
		\begin{itemize}
			\item Remove duplication caused by the addition of the new functionality.
			\item Make design changes to improve the overall solution.
			\item After each refactoring, rerun all the tests to ensure that they all still pass.
		\end{itemize}
		\item Repeat the cycle. Each cycle should be very short, and a typical hour should contain many Red/Green/Refactor
		cycles
	\end{itemize}
	
	Personally I adapted to work like this very quickly. It however has a drawback. One usually has to write 2 and more
	times more code than he would without testing. Each refactoring, that does not involve method renaming and extraction
	only, requires even more effort to update unit tests. This is why I used this approach only to test it within RESTful
	API, when developing import of admissions.
	
	Due to lack of time I continued without implementing unit tests any further. On the other hand, I encourage everyone to
	adopt this technique as it adds much more confidence and trust in developer's work.

	Frameworks that help to make unit testing a joy are e.g. JUnit and TestNG.
	
	There are several ways how to verify that production code is covered by unit tests. Very helpful tools, which are
	also available as Maven report plugins, are Cobertura, EMMA or JMockit code coverage. My personal experience is that
	this metrics should become a standard, when developing an important project. Therefore they should be set to
	100\% code coverage by unit tests, both line and branch.
	
	Another very helpful tool and Maven plugin is Findbugs. It compares the source code with a database of known best
	practices and warns the developer, if there are some places that need fixing or attention.

	\section{Integration Testing}
	
	\cite{msdnintegration}
	Integration testing is a logical extension of unit testing. In its simplest form, two units that have already been
	tested are combined into a component and the interface between them is tested. A component, in this sense, refers to an
	integrated aggregate of more than one unit. In a realistic scenario, many units are combined into components, which are
	in turn aggregated into even larger parts of the program. The idea is to test combinations of pieces and eventually
	expand the process to test your modules with those of other groups. Eventually all the modules making up a process are
	tested together. Beyond that, if the program is composed of more than one process, they should be tested in pairs
	rather than all at once.

	Integration testing identifies problems that occur when units are combined. By using a test plan that requires you to
	test each unit and ensure the viability of each before combining units, you know that any errors discovered when
	combining units are likely related to the interface between units. This method reduces the number of possibilities to a
	far simpler level of analysis.
	
	\subsection{Arquillian}
	
	It is a testing platform for the JVM that enables developers to easily create automated integration, functional and
	acceptance tests for Java middleware.
	
	Arquillian handles all the plumbing of container management, deployment and framework initialization. Moreover it
	covers all aspects of test execution, which entails:

	\begin{itemize}
		\item Managing the lifecycle of the container
		\item Bundling the test case, dependent classes and resources into a ShrinkWrap archive
		\item Deploying the archive to the container
		\item Enriching the test case by providing dependency injection and other declarative services
		\item Executing the tests inside the container
		\item Capturing the results and returning them to the test runner for reporting
		\item Integrates with familiar testing frameworks (e.g., JUnit, TestNG), allowing tests to be launched using existing
		IDE, Ant and Maven test plugins
	\end{itemize}
	
	RESTful API uses integration tests from two sources:
	
	\begin{itemize}
		\item Spring Roo generated for JPA Entities
		
		Each domain model is tested via Spring Roo unit and integration tests.
		\item handcrafted via Arquillian
		
		Again, due to lack of time, I just wanted to test it by myself and to demonstrate, how such tests should look like.
		This is why only a few integration tests can be found in RESTful API. They do not cover all endpoint and cover only
		positive flow.
	\end{itemize} 

	\section{Regression Testing}

	\section{Performance Testing}