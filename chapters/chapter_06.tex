\chapter{Testing}\label{cha:testing}

	\section{Unit Testing}
	
	\cite{msdnunit}
	The primary goal of unit testing is to take the smallest piece of testable software in the application, isolate it from
	the remainder of the code, and determine whether it behaves exactly as expected. Each unit is tested separately
	before integrating them into modules to test the interfaces between modules.
	
	In other words, unit tests test class by class, method by method. If a tested class or method has a dependency, the
	dependency has to be stubbed, mocked, faked, dummied or spied. The five just named together form a set of test doubles.

	\subsection{\gls{TDD}}
	
	\cite{msdntdd}
	TDD is an advanced technique of using automated unit tests to drive the design of software
	and force decoupling of dependencies. The result of using this practice is a comprehensive suite of unit tests that can
	be run at any time to provide feedback that the software is still working. This technique is heavily emphasized by
	those using Agile development methodologies.
	
	These steps should be followed when doing TDD right:
	
	\begin{itemize}
		\item Understand the requirements of the story, work item, or feature that you are working on
		\item \textbf{Red} Create a test and make it fail
		\begin{itemize}
			\item Imagine how the new code should be called and write the test as if the code already existed.
			\item Create the new production code stub. Write just enough code so that it compiles.
			\item Run the test. It should fail. This is a calibration measure to ensure that your test is calling the correct
			code and that the code is not working by accident. This is a meaningful failure, and you expect it to fail.
		\end{itemize}
		\item \textbf{Green} Make the test pass by any means necessary
		\begin{itemize}
			\item Write the production code to make the test pass. Keep it simple.
			\item Some advocate the hard-coding of the expected return value first to verify that the test correctly detects
			success. This varies from practitioner to practitioner.
			\item If you've written the code so that the test passes as intended, you are finished. You do not have to write more
			code speculatively. The test is the objective definition of \uv{done.} The phrase \gls{YAGNI} is often used to veto
			unnecessary work. If new functionality is still needed, then another test is needed. Make this one test pass and
			continue.
			\item When the test passes, you might want to run all tests up to this point to build confidence that everything else
			is still working.
		\end{itemize}
		\item \textbf{Refactor} Change the code to remove duplication in your project and to improve the design while ensuring
		that all tests still pass
		\begin{itemize}
			\item Remove duplication caused by the addition of the new functionality.
			\item Make design changes to improve the overall solution.
			\item After each refactoring, rerun all the tests to ensure that they all still pass.
		\end{itemize}
		\item Repeat the cycle. Each cycle should be very short, and a typical hour should contain many Red/Green/Refactor
		cycles
	\end{itemize}
	
	Personally I adapted to work like this very quickly. It however has a drawback. One usually has to write 2 and more
	times more code than he would without testing. Each refactoring, that does not involve method renaming and extraction
	only, requires even more effort to update unit tests. This is why I used this approach only to test it within RESTful
	API, when developing import of admissions.
	
	Due to lack of time I continued without implementing unit tests any further. On the other hand, I encourage everyone to
	adopt this technique as it adds much more confidence and trust in developer's work.

	Frameworks that help to make unit testing a joy are e.g. JUnit and TestNG.
	
	There are several ways how to verify that production code is covered by unit tests. Very helpful tools, which are
	also available as Maven report plugins, are Cobertura, EMMA or JMockit code coverage. If one is willing to pay for a
	commercial solution, Clover may be worth looking. These metrics should become a standard, when developing an important
	project. Therefore they should be set to 100\% code coverage by unit tests, both line and branch.
	
	Another very helpful tool and Maven plugin is Findbugs. It compares the source code with a database of known best
	practices and warns the developer, if there are some places that need fixing or attention.

	\section{Integration Testing}
	
	\cite{msdnintegration}
	Integration testing is a logical extension of unit testing. In its simplest form, two units that have already been
	tested are combined into a component and the interface between them is tested. A component, in this sense, refers to an
	integrated aggregate of more than one unit. In a realistic scenario, many units are combined into components, which are
	in turn aggregated into even larger parts of the program. The idea is to test combinations of pieces and eventually
	expand the process to test your modules with those of other groups. Eventually all the modules making up a process are
	tested together. Beyond that, if the program is composed of more than one process, they should be tested in pairs
	rather than all at once.

	Integration testing identifies problems that occur when units are combined. By using a test plan that requires you to
	test each unit and ensure the viability of each before combining units, you know that any errors discovered when
	combining units are likely related to the interface between units. This method reduces the number of possibilities to a
	far simpler level of analysis.
	
	\subsection{Arquillian}
	
	It is a testing platform for the JVM that enables developers to easily create automated integration, functional and
	acceptance tests for Java middleware.
	
	Arquillian handles all the plumbing of container management, deployment and framework initialization. Moreover it
	covers all aspects of test execution, which entails:

	\begin{itemize}
		\item Managing the lifecycle of the container
		\item Bundling the test case, dependent classes and resources into a ShrinkWrap archive
		\item Deploying the archive to the container
		\item Enriching the test case by providing dependency injection and other declarative services
		\item Executing the tests inside the container
		\item Capturing the results and returning them to the test runner for reporting
		\item Integrates with familiar testing frameworks (e.g., JUnit, TestNG), allowing tests to be launched using existing
		IDE, Ant and Maven test plugins
	\end{itemize}
	
	RESTful API uses integration tests from two sources:
	
	\begin{itemize}
		\item Spring Roo generated for JPA Entities
		
		Each domain model is tested via Spring Roo unit and integration tests.
		\item handcrafted via Arquillian
		
		Again, due to lack of time, I just wanted to test it by myself and to demonstrate, how such tests should look like.
		This is why only a few integration tests can be found in RESTful API. They do not cover all endpoints and cover only
		positive flow.
	\end{itemize} 

	\section{Acceptance Testing}
	
	A common form of acceptance testing consists of tests that exercise a consumer scenario. These tests emulate the
	application interface interactions. Important forms of acceptance testing follow.
	
	\subsection{Verification and Regression Testing}
	
	Verification testing is a regular check of application functionality. It is done from a perspective of a consumer. For
	RESTful API this means the client.
	
	Regression testing means testing that the software did not stop working. In other words: functionality that was
	working yesterday, is still working today. This can be understood as a set of Verification tests collected over time.
	
	\cite{msdnregression}
	Any time implementation within a program is modified, regression testing should be done. It usually means rerunning
	existing tests against the modified code to determine whether the changes break anything that worked prior to the
	change and by writing new tests where necessary. Adequate coverage without wasting time should be a primary
	consideration when conducting regression tests.
	
	Some strategies and factors to consider during this process include the following:
	
	\begin{itemize}
		\item Test fixed bugs promptly. The programmer might have handled the symptoms but not have gotten to the underlying
		cause.
		\item Watch for side effects of fixes. The bug itself might be fixed but the fix might create other bugs.
		\item Write a regression test for each bug fixed.
		\item If two or more tests are similar, determine which is less effective and get rid of it.
		\item Identify tests that the program consistently passes and archive them.
		\item Focus on functional issues, not those related to design.
		\item Make changes (small and large) to data and find any resulting corruption.
		\item Trace the effects of the changes on program memory.
	\end{itemize}
	
	\subsection{How to perform Regression Testing}
	
	First thing that should be kept in mind is that verification or regression should test the whole application in an
	environment similar to production. This is why it should be properly deployed and running.
	
	The only change between test and production environment should be testing data and testing without any other system
	dependencies. E.g. if a middleware is tested, it should be running in standalone mode - I do not want to test third
	party's application. This can influence two sides of the application. Input (North) or Backend interaction (South) or
	both.
	
	What does this mean for RESTful API? Because it is directly consumed by clients I do not need to care about the
	Northern part. Tests will just act like clients. More interesting is the Southern part. RESTful API communicates with
	Faculty's LDAP server and mail server. During Verification or Regression testing none of them should be contacted.
	
	Each RESTful API's service, that has a possibility to interact with a backend, contains an adapter implementation.
	Adapter is a \gls{DI}, which is injected during server startup and its implementation is configurable via
	\textbf{deployment.properties} file:
	
	\begin{verbatim}
	# DUMMY | PROD
	adapter.ldap=DUMMY
	adapter.pwd=PROD
	# DUMMY | UUID
	generator.string=UUID
	# EMAIL
	# true | false
	mail.disable=false
	\end{verbatim}
	
	A test suite for RESTful API is available. However, only for a couple of calls, again, due to lack of time.
	\verb|Apache Jmeter|\footnote{\cite{jmeter} The Apache JMeter\textsuperscript{\texttrademark} desktop application is open
	source software, a 100\% pure Java application designed to load test functional behavior and measure performance. It was
	originally designed for testing Web Applications but has since expanded to other test functions.} has been used for
	this purpose. The \textbf{.jmx} testing project is available as a part of RESTful API's source code
	\textbf{jmeter/admission\_tests.jmx}. It contains \uv{profiles} for various test cases, which can be enabled or
	disabled by selecting desired user variables set via JMeter GUI.
	
	\subsection{Performance Testing}
	
	\ref{itm:NF07}~NF07 talks about number of total and concurrent users that RESTful API should be able to handle. To
	verify such requirement, performance test is a right thing to be used.
	
	Performance test should, exactly as Verification or Regression tests, probe the application in an environment similar
	to production.
	It strictly focuses on application that the test scope is about. This means, there should be no delays due to backend
	communication, e-mail sending or similar action. The goal is to go through all the application layers from the top to
	the bottom and back: request - processing - response.
	
	Luckily, this is exactly what my regression test suite does. An exception might be a fact that
	\verb|Rainy day|\footnote{Non standard sequence of actions. Errors, exceptions, and less typical usage paths}
	scenarios are left out from performance test suite. Simply because they usually do not use all application layers and the request is usually refused during validation. This would give me distorted results and also would make error rate
	detection difficult under high load.
	
	Basically, performance test suite is a subset of regression test pack, where only positive
	\verb|Sunny day|\footnote{The typical sequence of actions and system responses} scenarios are included.
	
	RESTful API should be performance tested in three scenarios:
	
	\begin{itemize}
		\item \textbf{Admissions import}
		
		This is one time task, which imports large number of new data through a single RESTful API call. The performance test
		should verify that RESTful API does not slow down during the import - each Admission should be processed in constant
		time.
		\item \textbf{Entrance exam}
		
		Short term status, when large number of Admission results are put through RESTful API with many photos. Basically hits
		two endpoints. Should again verify that RESTful API does not slow down due to high data load.
		\item \textbf{Casual use}
		
		Test should simulate standard behavior of Web UI interface with focus on Registrations, session creation, Admission
		read, \ldots This is where the original NF07 is applicable and result of this scenario should reflect it in results.
	\end{itemize}
	
	Admissions import and Entrance exam scenarios have practically been tested during the development phase, when Web UI
	and Android teams tested their applications against RESTful API. No issues have been discovered.
	
	More important is to test, if the application can handle number of concurrent users described in NF07. No server errors
	should appear in logs and because performance test \textbf{.jmx} suite contains Sunny day scenarios only, no business
	errors should appear either. Well this is not completely true. Because there is only a single User Identity used for
	all operations, concurrent access will mess up session removal. For this and only this single case 401 and 404 HTTP
	error responses are not considered to be an error.
	
	Number of total users is not that interesting, because it is then just a matter of time to process them.
	
	\subsubsection{Infrastructure}
	
	When load testing an application a separate environment should be set up for application being tested and load
	injecting application. Luckily \gls{FIT} has good and quite powerful infrastructure available, and so RESTful API runs
	on its own virtual machine, which provides:
	
	% TODO
	\begin{itemize}
		\item AMD quad core CPU @ 2.0GHz
		\item 6GB RAM
	\end{itemize}
	
	\subsubsection{Scenarios and results}
	
	% TODO
	
	\subsection{User Acceptance Testing}
	
	This is a special case of Verification testing, when application is working exactly like in production environment, put
	together with Northern and Southern dependencies.
	
	The only difference when compared to real production use is that non-production data should be provided as an input
	during the test. Typically test users and some randomly generated data.
	
	For such scenario a small subset of Regression test pack can be selected, built on top of Sunny day scenarios.
	UAT should run very quickly. A common approach for RESTful API might be that each endpoint is hit with a single Sunny
	day request. Regression Test pack could be reused again or a quick automatic or manual test via Web UI is applicable
	too.
