\chapter{Testing}\label{cha:testing}

	\section{Unit Testing}
	
	\cite{msdnunit}
	The primary goal of unit testing is to take the smallest piece of testable software in the application, isolate it from
	the remainder of the code, and determine whether it behaves exactly as expected. Each unit is tested separately
	before integrating them into modules to test the interfaces between modules.
	
	In other words, unit tests test class by class, method by method. If a tested class or method has a dependency, the
	dependency has to be stubbed, mocked, faked, dummied or spied. The five just named together form a set of test doubles.

	\subsection{\gls{TDD}}
	
	\cite{msdntdd}
	TDD is an advanced technique of using automated unit tests to drive the design of software
	and force decoupling of dependencies. The result of using this practice is a comprehensive suite of unit tests that can
	be run at any time to provide feedback that the software is still working. This technique is heavily emphasized by
	those using Agile development methodologies.
	
	These steps should be followed when doing TDD right:
	
	\begin{itemize}
		\item Understand the requirements of the story, work item, or feature that you are working on
		\item \textbf{Red} Create a test and make it fail
		\begin{itemize}
			\item Imagine how the new code should be called and write the test as if the code already existed.
			\item Create the new production code stub. Write just enough code so that it compiles.
			\item Run the test. It should fail. This is a calibration measure to ensure that your test is calling the correct
			code and that the code is not working by accident. This is a meaningful failure, and you expect it to fail.
		\end{itemize}
		\item \textbf{Green} Make the test pass by any means necessary
		\begin{itemize}
			\item Write the production code to make the test pass. Keep it simple.
			\item Some advocate the hard-coding of the expected return value first to verify that the test correctly detects
			success. This varies from practitioner to practitioner.
			\item If you've written the code so that the test passes as intended, you are finished. You do not have to write more
			code speculatively. The test is the objective definition of \uv{done.} The phrase \gls{YAGNI} is often used to veto
			unnecessary work. If new functionality is still needed, then another test is needed. Make this one test pass and
			continue.
			\item When the test passes, you might want to run all tests up to this point to build confidence that everything else
			is still working.
		\end{itemize}
		\item \textbf{Refactor} Change the code to remove duplication in your project and to improve the design while ensuring
		that all tests still pass
		\begin{itemize}
			\item Remove duplication caused by the addition of the new functionality.
			\item Make design changes to improve the overall solution.
			\item After each refactoring, rerun all the tests to ensure that they all still pass.
		\end{itemize}
		\item Repeat the cycle. Each cycle should be very short, and a typical hour should contain many Red/Green/Refactor
		cycles
	\end{itemize}
	
	Personally I adapted to work like this very quickly. It however has a drawback. One usually has to write 2 and more
	times more code than he would without testing. Each refactoring, that does not involve method renaming and extraction
	only, requires even more effort to update unit tests. This is why I used this approach only to test it within RESTful
	API, when developing import of admissions.
	
	Due to lack of time I continued without implementing unit tests any further. On the other hand, I encourage everyone to
	adopt this technique as it adds much more confidence and trust in developer's work.

	Frameworks that help to make unit testing a joy are e.g. JUnit and TestNG.
	
	There are several ways how to verify that production code is covered by unit tests. Very helpful tools, which are
	also available as Maven report plugins, are Cobertura, JaCoCo (continuation of EMMA as a forked project) or JMockit
	code coverage.
	If one is willing to pay for a commercial solution, Clover may be worth looking. These metrics should become a standard, when developing an important
	project. Therefore they should be set to 100\% code coverage by unit tests, both line and branch.
	
	Another very helpful tool and Maven plugin is Findbugs. It compares the source code with a database of known best
	practices and warns the developer, if there are some places that need fixing or attention.

	\section{Integration Testing}
	
	\cite{msdnintegration}
	Integration testing is a logical extension of unit testing. In its simplest form, two units that have already been
	tested are combined into a component and the interface between them is tested. A component, in this sense, refers to an
	integrated aggregate of more than one unit. In a realistic scenario, many units are combined into components, which are
	in turn aggregated into even larger parts of the program. The idea is to test combinations of pieces and eventually
	expand the process to test your modules with those of other groups. Eventually all the modules making up a process are
	tested together. Beyond that, if the program is composed of more than one process, they should be tested in pairs
	rather than all at once.

	Integration testing identifies problems that occur when units are combined. By using a test plan that requires you to
	test each unit and ensure the viability of each before combining units, you know that any errors discovered when
	combining units are likely related to the interface between units. This method reduces the number of possibilities to a
	far simpler level of analysis.
	
	\subsection{Arquillian}
	
	It is a testing platform for the JVM that enables developers to easily create automated integration, functional and
	acceptance tests for Java middleware.
	
	Arquillian handles all the plumbing of container management, deployment and framework initialization. Moreover it
	covers all aspects of test execution, which entails:

	\begin{itemize}
		\item Managing the lifecycle of the container
		\item Bundling the test case, dependent classes and resources into a ShrinkWrap archive
		\item Deploying the archive to the container
		\item Enriching the test case by providing dependency injection and other declarative services
		\item Executing the tests inside the container
		\item Capturing the results and returning them to the test runner for reporting
		\item Integrates with familiar testing frameworks (e.g., JUnit, TestNG), allowing tests to be launched using existing
		IDE, Ant and Maven test plugins
	\end{itemize}
	
	RESTful API uses Spring Roo generated integration tests all for JPA Entities. Each domain model is tested via Spring
	Roo unit and integration tests.
	
	Due to lack of time I did not implement any integration tests using Arquillian, but if RESTful API should be further
	developed, this is a good framework to work with. 

	\section{Acceptance Testing}
	
	A common form of acceptance testing consists of tests that exercise a consumer scenario. These tests emulate the
	application interface interactions. Important forms of acceptance testing follow.
	
	\subsection{Verification and Regression Testing}
	
	Verification testing is a regular check of application functionality. It is done from a perspective of a consumer. For
	RESTful API this means the client.
	
	Regression testing means testing that the software did not stop working. In other words: functionality that was
	working yesterday, is still working today. This can be understood as a set of Verification tests collected over time.
	
	\cite{msdnregression}
	Any time implementation within a program is modified, regression testing should be done. It usually means rerunning
	existing tests against the modified code to determine whether the changes break anything that worked prior to the
	change and by writing new tests where necessary. Adequate coverage without wasting time should be a primary
	consideration when conducting regression tests.
	
	Some strategies and factors to consider during this process include the following:
	
	\begin{itemize}
		\item Test fixed bugs promptly. The programmer might have handled the symptoms but not have gotten to the underlying
		cause.
		\item Watch for side effects of fixes. The bug itself might be fixed but the fix might create other bugs.
		\item Write a regression test for each bug fixed.
		\item If two or more tests are similar, determine which is less effective and get rid of it.
		\item Identify tests that the program consistently passes and archive them.
		\item Focus on functional issues, not those related to design.
		\item Make changes (small and large) to data and find any resulting corruption.
		\item Trace the effects of the changes on program memory.
	\end{itemize}
	
	\subsection{How to perform Regression Testing}
	
	First thing that should be kept in mind is that verification or regression should test the whole application in an
	environment similar to production. This is why it should be properly deployed and running.
	
	The only change between test and production environment should be testing data and testing without any other system
	dependencies. E.g. if a middleware is tested, it should be running in standalone mode - I do not want to test third
	party's application. This can influence two sides of the application. Input (North) or Backend interaction (South) or
	both.
	
	What does this mean for RESTful API? Because it is directly consumed by clients I do not need to care about the
	Northern part. Tests will just act like clients. More interesting is the Southern part. RESTful API communicates with
	Faculty's LDAP server and mail server. During Verification or Regression testing none of them should be contacted.
	
	Each RESTful API's service, that has a possibility to interact with a backend, contains an adapter implementation.
	Adapter is a \gls{DI}, which is injected during server startup and its implementation is configurable via
	\textbf{deployment.properties} file:
	
	\begin{verbatim}
	# DUMMY | PROD
	adapter.ldap=DUMMY
	adapter.pwd=PROD
	# DUMMY | UUID
	generator.string=UUID
	# EMAIL
	# true | false
	mail.disable=false
	\end{verbatim}
	
	A test suite for RESTful API is available. It is split into Regression pack and Load testing pack. The first one
	contains multiple test scenarios, which were collected during the development. They verify RESTful API in a matter of
	its basic functionality. \verb|Apache Jmeter|\footnote{\cite{jmeter} The Apache JMeter\textsuperscript{\texttrademark}
	desktop application is open source software, a 100\% pure Java application designed to load test functional behavior and measure performance. It was originally designed for testing Web Applications but has since expanded to other test functions.} has been used for
	this purpose. The \textbf{.jmx} testing project is available as a part of RESTful API's source code
	\textbf{jmeter/admission\_tests.jmx}. It contains \uv{profiles} for various test cases, which can be enabled or
	disabled by selecting desired user variables set via JMeter GUI.
	
	\subsection{Performance Testing}
	
	\ref{itm:NF07}~NF07 talks about number of total and concurrent users that RESTful API should be able to handle. To
	verify this requirement I ran a simple performance test.
	
	Performance test should probe the application in an environment similar to production.
	It strictly focuses on the application that the test scope is about. This means, there should be no delays due to
	the backend communication, e-mail sending or similar action. The goal is to go through all the application layers from
	the top to the bottom and back: request - processing - response. It can also be used to find memory leaks in the
	application, but this is out of scope of my work.
	
	Load testing pack of my JMeter test suite was created specifically for this task. A fact is
	that \verb|Rainy day|\footnote{Non standard sequence of actions. Errors, exceptions, and less typical usage paths}
	scenarios are left out from Load testing pack, which makes it different from the Regression pack. Simply because they
	usually do not use all application layers and the request is usually refused during validation. This would give me
	distorted results and also would make error rate detection difficult under high load.
	
	In other words, the Load testing pack is a subset of Regression test pack, where only positive
	\verb|Sunny day|\footnote{The typical sequence of actions and system responses} scenarios are included.
	
	RESTful API should be performance tested in three scenarios:
	
	\begin{itemize}
		\item \textbf{Admissions import}
		
		This is one time task, which imports large number of new data through a single RESTful API call. The performance test
		should verify that RESTful API does not slow down during the import - each Admission should be processed in constant
		time.
		\item \textbf{Entrance exam}
		
		Short term status, when large number of Admission results are put through RESTful API with many photos. Basically hits
		two endpoints. Should again verify that RESTful API does not slow down due to high data load.
		\item \textbf{Casual use}
		
		Test should simulate standard behavior of Web UI interface with focus on Registrations, session creation, Admission
		read, \ldots This is where the original NF07 is applicable and result of this scenario should reflect it in results.
	\end{itemize}
	
	Admissions import and Entrance exam scenarios have practically been tested during the development phase, when Web UI
	and Android teams tested their applications against RESTful API. No issues have been discovered.
	
	More important is to test, whether the application can handle number of concurrent users described in NF07. No server
	errors should appear in logs and because performance test \textbf{.jmx} suite contains Sunny day scenarios only, no business
	errors should appear either. This is not completely true. Because there is only a single User Identity used for
	all operations, concurrent access will mess up session removal. For this and only this single case 401 and 404 HTTP
	error responses are not considered to be an error.
	
	Number of total users is not that interesting, because it is then just a matter of time to process them.
	
	\subsubsection{Infrastructure}
	
	When load testing an application a separate environment should be set up for both, the application being tested and
	the load injecting application. Luckily \gls{FIT} provides quite powerful infrastructure, and so RESTful
	API runs on its own virtual machine, which contains:
	
	\begin{itemize}
		\item AMD quad core CPU @ 2.0GHz
		\item 6GB RAM
	\end{itemize}
	
	This configuration should be sufficient. For production use it should, however, be duplicated and load balanced.
	
	\subsubsection{Scenarios and results}
	
	For quick performance evaluation I decided to create statistics of all calls from RESTful API logs found on shared
	virtual server. These were collected for several days.
	This is what I declared to be a casual use. Load test is then set up to simulate behavior of Web UI and Android teams
	during their development.
	
	Load test was then performed in two phases:
	
	\begin{itemize}
		\item 50\% load, i.e. 125 concurrent threads ($\approx$ users)
		\item 100\% load, i.e. 250 concurrent threads ($\approx$ users)  
	\end{itemize}
	
	\begin{table}[h]\centering
	 	\begin{minipage}{12.9cm}
		\begin{tabular}{l|r|p{5.7cm}}
		\hline
		Identifier & Ratio & Description\\
		\hline
		throughput.viewproperties &	5 & Static page, lists all server properties \\
		throughput.admissions & 20 & Retrieve list of admissions, paginated \\
		throughput.admission.crud & 5 & Admission CRUD scenario \\
		throughput.user.userIdentity & 60 & Obtain all user information (session, roles, \ldots) \\
		throughput.programs & 20 & Retrieve list of programs, paginated \\
		throughput.programme.crud & 10 & Programme CRUD scenario \\
		throughput.terms & 20 & Retrieve list of terms \\
		throughput.term.crud & 10 & Term CRUD scenario \\
		\end{tabular}
	    \renewcommand{\footnoterule}{}
	    \end{minipage}
	\caption{Load distribution among calls}
	\label{load_distribution}
	\end{table}
	
	Tests were run for approximately 15 minutes in each setup. During the test I found out, that the most resource
	consuming operation is the import of a new Admission. This implies creating all its child entities or assigning the
	existing ones (RESTful API does not create duplicates for entities, that can be shared, e.g. Country, City, \ldots).
	Out of the interest I did one Load test round without Admission import. This implies more GET than PUT or POST
	operations.
	
	Detailed result analysis can be found in Appendix~\ref{app:performance}. They prove that RESTful API is capable of
	handling the request amount of concurrent users. Error ratio is zero, which means that all requests have been
	successfully processed. Although it lacks responsibility a bit. This leaves space for further optimisation. Places
	worth to look at involve:
	
	\begin{itemize}
		\item JPA and Hibernate tuning
		\item Second level database cache for entities
		\item MySQL, JVM, Tomcat and virtual machine tuning
		\item Multiple Tomcat nodes with load balancing 
	\end{itemize}
	
	This list is not even half complete, but may be considered as a hint at the beginning. Each of the items listed above
	can possibly lead to new discoveries and possible enhancements.
	
	\subsection{User Acceptance Testing}
	
	This is a special case of Verification testing, when application is working exactly like in production environment, put
	together with Northern and Southern dependencies.
	
	The only difference when compared to real production use is that non-production data should be provided as an input
	during the test. Typically test users and some randomly generated data.
	
	For such scenario a small subset of Regression test pack can be selected, built on top of Sunny day scenarios.
	UAT should run very quickly. A common approach for RESTful API might be that each endpoint is hit with a single Sunny
	day request. Regression Test pack could be reused again or a quick automatic or manual test via Web UI is applicable
	too.
