\chapter{Testing}\label{cha:testing}

	\section{Unit Testing}
	
	\cite{msdnunit}
	The primary goal of unit testing is to take the smallest piece of testable software in the application, isolate it from
	the remainder of the code, and determine whether it behaves exactly as expected. Each unit is tested separately
	before integrating them into modules to test the interfaces between modules.
	
	In other words, unit tests test class by class, method by method. If a tested class or method has a dependency, the
	dependency has to be stubbed, mocked, faked, dummied or spied. The five just named together form a set of test doubles.

	\subsection{\gls{TDD}}
	
	\cite{msdntdd}
	TDD is an advanced technique of using automated unit tests to drive the design of software
	and force decoupling of dependencies. The result of using this practice is a comprehensive suite of unit tests that can
	be run at any time to provide feedback that the software is still working. This technique is heavily emphasized by
	those using Agile development methodologies.
	
	These steps should be followed when doing TDD right:
	
	\begin{itemize}
		\item Understand the requirements of the story, work item, or feature that you are working on
		\item \textbf{Red} Create a test and make it fail
		\begin{itemize}
			\item Imagine how the new code should be called and write the test as if the code already existed.
			\item Create the new production code stub. Write just enough code so that it compiles.
			\item Run the test. It should fail. This is a calibration measure to ensure that your test is calling the correct
			code and that the code is not working by accident. This is a meaningful failure, and you expect it to fail.
		\end{itemize}
		\item \textbf{Green} Make the test pass by any means necessary
		\begin{itemize}
			\item Write the production code to make the test pass. Keep it simple.
			\item Some advocate the hard-coding of the expected return value first to verify that the test correctly detects
			success. This varies from practitioner to practitioner.
			\item If you've written the code so that the test passes as intended, you are finished. You do not have to write more
			code speculatively. The test is the objective definition of \uv{done.} The phrase \gls{YAGNI} is often used to veto
			unnecessary work. If new functionality is still needed, then another test is needed. Make this one test pass and
			continue.
			\item When the test passes, you might want to run all tests up to this point to build confidence that everything else
			is still working.
		\end{itemize}
		\item \textbf{Refactor} Change the code to remove duplication in your project and to improve the design while ensuring
		that all tests still pass
		\begin{itemize}
			\item Remove duplication caused by the addition of the new functionality.
			\item Make design changes to improve the overall solution.
			\item After each refactoring, rerun all the tests to ensure that they all still pass.
		\end{itemize}
		\item Repeat the cycle. Each cycle should be very short, and a typical hour should contain many Red/Green/Refactor
		cycles
	\end{itemize}
	
	Personally I adapted to work like this very quickly. It however has a drawback. One usually has to write 2 and more
	times more code than he would without testing. Each refactoring, that does not involve method renaming and extraction
	only, requires even more effort to update unit tests. This is why I used this approach only to test it within RESTful
	API, when developing import of admissions.
	
	Due to lack of time I continued without implementing unit tests any further. On the other hand, I encourage everyone to
	adopt this technique as it adds much more confidence and trust in developer's work.

	Frameworks that help to make unit testing a joy are e.g. JUnit and TestNG.
	
	There are several ways how to verify that production code is covered by unit tests. Very helpful tools, which are
	also available as Maven report plugins, are Cobertura, EMMA or JMockit code coverage. If one is willing to pay for a
	commercial solution, Clover may be worth looking. These metrics should become a standard, when developing an important
	project. Therefore they should be set to 100\% code coverage by unit tests, both line and branch.
	
	Another very helpful tool and Maven plugin is Findbugs. It compares the source code with a database of known best
	practices and warns the developer, if there are some places that need fixing or attention.

	\section{Integration Testing}
	
	\cite{msdnintegration}
	Integration testing is a logical extension of unit testing. In its simplest form, two units that have already been
	tested are combined into a component and the interface between them is tested. A component, in this sense, refers to an
	integrated aggregate of more than one unit. In a realistic scenario, many units are combined into components, which are
	in turn aggregated into even larger parts of the program. The idea is to test combinations of pieces and eventually
	expand the process to test your modules with those of other groups. Eventually all the modules making up a process are
	tested together. Beyond that, if the program is composed of more than one process, they should be tested in pairs
	rather than all at once.

	Integration testing identifies problems that occur when units are combined. By using a test plan that requires you to
	test each unit and ensure the viability of each before combining units, you know that any errors discovered when
	combining units are likely related to the interface between units. This method reduces the number of possibilities to a
	far simpler level of analysis.
	
	\subsection{Arquillian}
	
	It is a testing platform for the JVM that enables developers to easily create automated integration, functional and
	acceptance tests for Java middleware.
	
	Arquillian handles all the plumbing of container management, deployment and framework initialization. Moreover it
	covers all aspects of test execution, which entails:

	\begin{itemize}
		\item Managing the lifecycle of the container
		\item Bundling the test case, dependent classes and resources into a ShrinkWrap archive
		\item Deploying the archive to the container
		\item Enriching the test case by providing dependency injection and other declarative services
		\item Executing the tests inside the container
		\item Capturing the results and returning them to the test runner for reporting
		\item Integrates with familiar testing frameworks (e.g., JUnit, TestNG), allowing tests to be launched using existing
		IDE, Ant and Maven test plugins
	\end{itemize}
	
	RESTful API uses integration tests from two sources:
	
	\begin{itemize}
		\item Spring Roo generated for JPA Entities
		
		Each domain model is tested via Spring Roo unit and integration tests.
		\item handcrafted via Arquillian
		
		Again, due to lack of time, I just wanted to test it by myself and to demonstrate, how such tests should look like.
		This is why only a few integration tests can be found in RESTful API. They do not cover all endpoints and cover only
		positive flow.
	\end{itemize} 

	\section{Regression Testing}
	
	Regression testing means testing, that the software did not stop working. In other words: functionality that was
	working yesterday, is still working today.
	
	\cite{msdnregression}
	Any time implementation within a program is modified, regression testing should be done. It usually means rerunning
	existing tests against the modified code to determine whether the changes break anything that worked prior to the
	change and by writing new tests where necessary. Adequate coverage without wasting time should be a primary
	consideration when conducting regression tests.
	
	Some strategies and factors to consider during this process include the following:
	
	\begin{itemize}
		\item Test fixed bugs promptly. The programmer might have handled the symptoms but not have gotten to the underlying
		cause.
		\item Watch for side effects of fixes. The bug itself might be fixed but the fix might create other bugs.
		\item Write a regression test for each bug fixed.
		\item If two or more tests are similar, determine which is less effective and get rid of it.
		\item Identify tests that the program consistently passes and archive them.
		\item Focus on functional issues, not those related to design.
		\item Make changes (small and large) to data and find any resulting corruption.
		\item Trace the effects of the changes on program memory.
	\end{itemize}
	
	\subsection{How to perform Regression Testing}
	
	First thing that should be kept in mind is that regression should test the whole application in an environment similar
	to production. This is why it should be properly deployed and running.
	
	The only change between test and production environment should be testing data and testing without any other system
	dependencies. E.g. if a middleware is tested, it should be running in standalone mode - I do not want to test third
	party's application. This can influence two sides of the application. Input (North) or Backend interaction (South) or
	both.
	
	What does this mean for RESTful API? Because it is directly consumed by clients I do not need to care about the
	Northern part. Tests will just act like clients. More interesting is the Southern part. RESTful API communicates with
	Faculty's LDAP server and mail server. During Regression Testing none of them should be contacted.
	
	Each RESTful API's service that has a possibility to interact with a backend contains an adapter implementation.
	Adapter is a \gls{DI}, which is injected during server startup and its implementation is configurable via
	\textbf{deployment.properties} file:
	
	\begin{verbatim}
	# DUMMY | PROD
	adapter.ldap=DUMMY
	adapter.pwd=PROD
	# DUMMY | UUID
	generator.string=UUID
	# EMAIL
	# true | false
	mail.disable=false
	\end{verbatim}

	\section{Performance Testing}
	
	\section{Acceptance Testing}